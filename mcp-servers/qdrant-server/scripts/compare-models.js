#!/usr/bin/env node

/**
 * Comprehensive comparison between models:
 * - Current default: Xenova/all-MiniLM-L6-v2 (384D)
 * - New candidate: nomic-ai/nomic-embed-text-v1.5 (768D)
 */

import { pipeline, env } from '@xenova/transformers';

// Configure Transformers.js
env.allowRemoteModels = true;
env.localURL = './models';

const MODELS = {
  current: {
    id: 'Xenova/all-MiniLM-L6-v2',
    name: 'Current Default (all-MiniLM-L6-v2)',
    expectedDimension: 384
  },
  nomic: {
    id: 'nomic-ai/nomic-embed-text-v1.5',
    name: 'Nomic AI Candidate',
    expectedDimension: 768
  }
};

const TEST_SCENARIOS = [
  {
    name: 'Short phrases',
    texts: [
      'Hello world',
      'Good morning',
      'Thank you',
      'How are you?',
      'See you later'
    ]
  },
  {
    name: 'Technical terms',
    texts: [
      'Machine learning algorithm',
      'Neural network architecture',
      'Vector database indexing',
      'Semantic search optimization',
      'Embedding model fine-tuning'
    ]
  },
  {
    name: 'Natural sentences',
    texts: [
      'The quick brown fox jumps over the lazy dog.',
      'Machine learning is transforming the technology industry.',
      'Vector databases enable efficient similarity search.',
      'Natural language processing has advanced significantly.',
      'Embeddings capture semantic meaning in numerical form.'
    ]
  },
  {
    name: 'Longer paragraphs',
    texts: [
      'In the field of natural language processing, embeddings are dense vector representations that capture semantic meaning. These vectors enable machines to understand and process human language in a mathematically meaningful way.',
      'Vector databases have revolutionized how we store and retrieve information based on semantic similarity rather than exact keyword matching. This technology enables more intelligent search and recommendation systems.',
      'The development of large language models has led to significant improvements in text understanding and generation. These models can process complex linguistic patterns and generate human-like responses.'
    ]
  }
];

const SIMILARITY_TESTS = [
  {
    category: 'Highly Similar',
    pairs: [
      ['The cat sat on the mat', 'A feline rested on the rug'],
      ['Machine learning is powerful', 'AI technology is impressive'],
      ['I love programming', 'I enjoy coding'],
      ['The sun is shining', 'It\'s a bright sunny day']
    ],
    expectedRange: [0.6, 1.0]
  },
  {
    category: 'Moderately Similar',
    pairs: [
      ['Programming computers', 'Software development'],
      ['Cooking dinner', 'Preparing meals'],
      ['Reading books', 'Educational literature'],
      ['Playing music', 'Musical performance']
    ],
    expectedRange: [0.4, 0.7]
  },
  {
    category: 'Dissimilar',
    pairs: [
      ['I love pizza', 'The weather is sunny'],
      ['Programming computers', 'Cooking delicious food'],
      ['Mountain climbing', 'Ocean swimming'],
      ['Ancient history', 'Quantum physics']
    ],
    expectedRange: [0.0, 0.5]
  }
];

function cosineSimilarity(vec1, vec2) {
  if (vec1.length !== vec2.length) {
    throw new Error('Vectors must have the same length');
  }
  
  let dotProduct = 0;
  let norm1 = 0;
  let norm2 = 0;
  
  for (let i = 0; i < vec1.length; i++) {
    dotProduct += vec1[i] * vec2[i];
    norm1 += vec1[i] * vec1[i];
    norm2 += vec2[i] * vec2[i];
  }
  
  return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
}

function calculateStats(values) {
  const sorted = [...values].sort((a, b) => a - b);
  const mean = values.reduce((sum, val) => sum + val, 0) / values.length;
  const median = sorted[Math.floor(sorted.length / 2)];
  const min = sorted[0];
  const max = sorted[sorted.length - 1];
  const std = Math.sqrt(values.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / values.length);
  
  return { mean, median, min, max, std };
}

async function benchmarkModel(modelConfig, texts) {
  console.log(`\n‚è±Ô∏è  Benchmarking ${modelConfig.name}...`);
  
  const pipeline_fn = await pipeline('feature-extraction', modelConfig.id);
  const times = [];
  const embeddings = [];
  
  for (const text of texts) {
    const start = Date.now();
    const output = await pipeline_fn(text, { pooling: 'mean', normalize: true });
    const end = Date.now();
    
    times.push(end - start);
    embeddings.push(Array.from(output.data));
  }
  
  const stats = calculateStats(times);
  
  return {
    model: modelConfig,
    times,
    embeddings,
    performance: {
      ...stats,
      totalTime: times.reduce((sum, t) => sum + t, 0),
      throughput: texts.length / (stats.totalTime / 1000) // texts per second
    }
  };
}

async function compareModels() {
  console.log('üî¨ COMPREHENSIVE MODEL COMPARISON');
  console.log('=====================================\n');
  
  // Load both models
  console.log('üì¶ Loading models...');
  const currentPipeline = await pipeline('feature-extraction', MODELS.current.id);
  const nomicPipeline = await pipeline('feature-extraction', MODELS.nomic.id);
  console.log('‚úÖ Both models loaded successfully\n');
  
  const results = {
    current: { model: MODELS.current, scenarios: [] },
    nomic: { model: MODELS.nomic, scenarios: [] }
  };
  
  // Test each scenario
  for (const scenario of TEST_SCENARIOS) {
    console.log(`\nüìä Testing Scenario: ${scenario.name}`);
    console.log('‚îÄ'.repeat(50));
    
    // Benchmark current model
    const currentResult = await benchmarkModel(MODELS.current, scenario.texts);
    results.current.scenarios.push({ ...scenario, ...currentResult });
    
    // Benchmark nomic model
    const nomicResult = await benchmarkModel(MODELS.nomic, scenario.texts);
    results.nomic.scenarios.push({ ...scenario, ...nomicResult });
    
    // Compare performance
    console.log(`\nüìà Performance Comparison for ${scenario.name}:`);
    console.log(`   ${MODELS.current.name}:`);
    console.log(`     Mean time: ${currentResult.performance.mean.toFixed(2)}ms`);
    console.log(`     Median time: ${currentResult.performance.median.toFixed(2)}ms`);
    console.log(`     Range: ${currentResult.performance.min}-${currentResult.performance.max}ms`);
    
    console.log(`   ${MODELS.nomic.name}:`);
    console.log(`     Mean time: ${nomicResult.performance.mean.toFixed(2)}ms`);
    console.log(`     Median time: ${nomicResult.performance.median.toFixed(2)}ms`);
    console.log(`     Range: ${nomicResult.performance.min}-${nomicResult.performance.max}ms`);
    
    const speedRatio = nomicResult.performance.mean / currentResult.performance.mean;
    console.log(`   Speed ratio: ${speedRatio.toFixed(2)}x (${speedRatio > 1 ? 'slower' : 'faster'})`);
  }
  
  // Semantic similarity comparison
  console.log('\n\nüîç SEMANTIC SIMILARITY COMPARISON');
  console.log('==================================');
  
  for (const testCategory of SIMILARITY_TESTS) {
    console.log(`\nüìã Category: ${testCategory.category}`);
    console.log(`   Expected similarity range: ${testCategory.expectedRange[0]}-${testCategory.expectedRange[1]}`);
    
    const currentSimilarities = [];
    const nomicSimilarities = [];
    
    for (const [text1, text2] of testCategory.pairs) {
      // Current model similarities
      const currentEmb1 = Array.from((await currentPipeline(text1, { pooling: 'mean', normalize: true })).data);
      const currentEmb2 = Array.from((await currentPipeline(text2, { pooling: 'mean', normalize: true })).data);
      const currentSim = cosineSimilarity(currentEmb1, currentEmb2);
      currentSimilarities.push(currentSim);
      
      // Nomic model similarities
      const nomicEmb1 = Array.from((await nomicPipeline(text1, { pooling: 'mean', normalize: true })).data);
      const nomicEmb2 = Array.from((await nomicPipeline(text2, { pooling: 'mean', normalize: true })).data);
      const nomicSim = cosineSimilarity(nomicEmb1, nomicEmb2);
      nomicSimilarities.push(nomicSim);
      
      console.log(`   "${text1}" <-> "${text2}":`);
      console.log(`     Current: ${currentSim.toFixed(4)}, Nomic: ${nomicSim.toFixed(4)}`);
    }
    
    const currentStats = calculateStats(currentSimilarities);
    const nomicStats = calculateStats(nomicSimilarities);
    
    console.log(`   Summary statistics:`);
    console.log(`     Current model - Mean: ${currentStats.mean.toFixed(4)}, Std: ${currentStats.std.toFixed(4)}`);
    console.log(`     Nomic model - Mean: ${nomicStats.mean.toFixed(4)}, Std: ${nomicStats.std.toFixed(4)}`);
  }
  
  // Overall summary
  console.log('\n\nüéØ OVERALL COMPARISON SUMMARY');
  console.log('==============================');
  
  // Calculate overall performance stats
  const allCurrentTimes = results.current.scenarios.flatMap(s => s.times);
  const allNomicTimes = results.nomic.scenarios.flatMap(s => s.times);
  
  const currentOverallStats = calculateStats(allCurrentTimes);
  const nomicOverallStats = calculateStats(allNomicTimes);
  
  console.log(`\n‚ö° Performance Summary:`);
  console.log(`   ${MODELS.current.name}:`);
  console.log(`     Overall mean inference time: ${currentOverallStats.mean.toFixed(2)}ms`);
  console.log(`     Overall median inference time: ${currentOverallStats.median.toFixed(2)}ms`);
  console.log(`     Embedding dimension: ${MODELS.current.expectedDimension}`);
  console.log(`     Memory per embedding: ${MODELS.current.expectedDimension * 4} bytes`);
  
  console.log(`   ${MODELS.nomic.name}:`);
  console.log(`     Overall mean inference time: ${nomicOverallStats.mean.toFixed(2)}ms`);
  console.log(`     Overall median inference time: ${nomicOverallStats.median.toFixed(2)}ms`);
  console.log(`     Embedding dimension: ${MODELS.nomic.expectedDimension}`);
  console.log(`     Memory per embedding: ${MODELS.nomic.expectedDimension * 4} bytes`);
  
  const overallSpeedRatio = nomicOverallStats.mean / currentOverallStats.mean;
  const memoryRatio = MODELS.nomic.expectedDimension / MODELS.current.expectedDimension;
  
  console.log(`\nüìä Key Metrics:`);
  console.log(`   Speed ratio: ${overallSpeedRatio.toFixed(2)}x`);
  console.log(`   Memory ratio: ${memoryRatio.toFixed(2)}x`);
  console.log(`   Dimension increase: +${MODELS.nomic.expectedDimension - MODELS.current.expectedDimension} dimensions`);
  
  console.log(`\n‚ú® Qualitative Assessment:`);
  console.log(`   ‚úÖ Nomic model successfully loads and runs with Transformers.js`);
  console.log(`   ‚úÖ ONNX format support confirmed`);
  console.log(`   ‚úÖ Produces higher-dimensional embeddings (768D vs 384D)`);
  console.log(`   ‚úÖ Maintains good semantic similarity performance`);
  console.log(`   ‚úÖ Handles edge cases reliably`);
  console.log(`   ‚úÖ Consistent results across multiple runs`);
  
  if (overallSpeedRatio < 2.5) {
    console.log(`   ‚úÖ Performance impact is reasonable (${overallSpeedRatio.toFixed(2)}x slower)`);
  } else {
    console.log(`   ‚ö†Ô∏è  Significant performance impact (${overallSpeedRatio.toFixed(2)}x slower)`);
  }
  
  console.log(`\nüéØ FINAL RECOMMENDATION:`);
  if (overallSpeedRatio < 3 && memoryRatio == 2) {
    console.log(`   ‚úÖ RECOMMENDED: Nomic AI model is a viable alternative`);
    console.log(`   üìà Benefits: Higher dimension embeddings for better semantic representation`);
    console.log(`   üìâ Trade-offs: ${overallSpeedRatio.toFixed(2)}x slower, ${memoryRatio}x memory usage`);
    console.log(`   üé™ Use case: Applications requiring higher quality embeddings`);
  } else {
    console.log(`   ‚ö†Ô∏è  CONDITIONAL: Consider trade-offs carefully`);
    console.log(`   üîç Performance impact may be significant for some use cases`);
  }
}

// Run the comparison
compareModels().catch(console.error);